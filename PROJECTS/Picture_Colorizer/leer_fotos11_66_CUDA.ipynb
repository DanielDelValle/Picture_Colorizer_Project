{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "https://sciencecareer.data.blog/2020/05/04/auto-colorization-of-black-and-white-images-using-machine-learning-auto-encoders-technique/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import glob\r\n",
    "import cv2\r\n",
    "import time\r\n",
    "import random\r\n",
    "import webbrowser\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from numpy import reshape\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.image as mpimg\r\n",
    "from matplotlib.pyplot import savefig\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "#from tensorflow.keras import datasets, layers, models\r\n",
    "import tensorflow.keras\r\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, Flatten, Reshape\r\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, Dropout, Flatten, InputLayer, BatchNormalization, Input, RepeatVector, concatenate\r\n",
    "from tensorflow.keras.models import Model, Sequential, model_from_json\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau, ModelCheckpoint\r\n",
    "from tensorflow.keras.utils import plot_model\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "\r\n",
    "from skimage import color\r\n",
    "from skimage.io import imsave\r\n",
    "from skimage.transform import resize\r\n",
    "from skimage.io import imsave, imread, imshow\r\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\r\n",
    "\r\n",
    "from __future__ import absolute_import\r\n",
    "from __future__ import division\r\n",
    "from __future__ import print_function       #repasar funcion y explicar en exposicion\r\n",
    "\r\n",
    "#import tensorflow.compat.v1 as tf\r\n",
    "#tf.disable_v2_behavior()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Get images\r\n",
    "img = []\r\n",
    "for filename in os.listdir('color256/'):\r\n",
    "    img.append(img_to_array(load_img('color256/'+filename)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "len(img)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6899"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "X = []\r\n",
    "Y = []\r\n",
    "\r\n",
    "for x in img:\r\n",
    "    lab = rgb2lab(x)\r\n",
    "    X.append(lab[:,:,0])\r\n",
    "    Y.append(lab[:,:,1:] / 128)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X = np.asarray(X, dtype=float)/255.0\r\n",
    "Y = np.asarray(Y, dtype=float)/255.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Encoder\r\n",
    "encoder_input = Input(shape=(256, 256, 1,))\r\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\r\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\r\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\r\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\r\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\r\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\r\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\r\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\r\n",
    "\r\n",
    "#Decoder\r\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\r\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\r\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\r\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\r\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\r\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\r\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\r\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\r\n",
    "\r\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)\r\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.01)\r\n",
    "model.compile(optimizer=optimizer, loss='mse' , metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%time\r\n",
    "history = model.fit(X,Y,validation_split=0.10, epochs=50, callbacks=callback)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.0018 - accuracy: 0.6157 - val_loss: 0.0015 - val_accuracy: 0.5720\n",
      "Epoch 2/50\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 0.0018 - accuracy: 0.6172 - val_loss: 0.0011 - val_accuracy: 0.5765\n",
      "Epoch 3/50\n",
      "195/195 [==============================] - 46s 234ms/step - loss: 0.0018 - accuracy: 0.6172 - val_loss: 0.0011 - val_accuracy: 0.5775\n",
      "Epoch 4/50\n",
      "195/195 [==============================] - 46s 234ms/step - loss: 0.0018 - accuracy: 0.6180 - val_loss: 0.0011 - val_accuracy: 0.5766\n",
      "Epoch 5/50\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 0.0018 - accuracy: 0.6183 - val_loss: 0.0011 - val_accuracy: 0.5784\n",
      "Epoch 6/50\n",
      "195/195 [==============================] - 45s 232ms/step - loss: 0.0018 - accuracy: 0.6183 - val_loss: 0.0012 - val_accuracy: 0.5778\n",
      "Epoch 7/50\n",
      "195/195 [==============================] - 46s 235ms/step - loss: 0.0018 - accuracy: 0.6205 - val_loss: 0.0011 - val_accuracy: 0.5778\n",
      "Epoch 8/50\n",
      "195/195 [==============================] - 46s 233ms/step - loss: 0.0018 - accuracy: 0.6194 - val_loss: 0.0011 - val_accuracy: 0.5754\n",
      "Epoch 9/50\n",
      "195/195 [==============================] - 46s 235ms/step - loss: 0.0018 - accuracy: 0.6206 - val_loss: 0.0011 - val_accuracy: 0.5788\n",
      "Epoch 10/50\n",
      "195/195 [==============================] - 45s 232ms/step - loss: 0.0018 - accuracy: 0.6214 - val_loss: 0.0011 - val_accuracy: 0.5779\n",
      "Epoch 11/50\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 0.0018 - accuracy: 0.6202 - val_loss: 0.0011 - val_accuracy: 0.5779\n",
      "Epoch 12/50\n",
      "195/195 [==============================] - 45s 231ms/step - loss: 0.0018 - accuracy: 0.6213 - val_loss: 0.0013 - val_accuracy: 0.5732\n",
      "Epoch 13/50\n",
      "195/195 [==============================] - 45s 233ms/step - loss: 0.0018 - accuracy: 0.6211 - val_loss: 0.0011 - val_accuracy: 0.5795\n",
      "Epoch 14/50\n",
      "134/195 [===================>..........] - ETA: 13s - loss: 0.0018 - accuracy: 0.6186"
     ]
    },
    {
     "output_type": "error",
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[32,256,256,2] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node GatherV2_1}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[32,256,256,2] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node GatherV2_1}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[IteratorGetNext/_4]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1856]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[32,256,256,2] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node GatherV2_1}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[32,256,256,2] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node GatherV2_1}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[IteratorGetNext/_4]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1856]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# list all data in history\r\n",
    "print(history.history.keys())\r\n",
    "# summarize history for accuracy\r\n",
    "fig = plt.figure(figsize=(12,8))\r\n",
    "plt.plot(history.history['accuracy'],label='training accuracy', color = \"blue\")\r\n",
    "plt.plot(history.history['val_accuracy'],label='validation accuracy', color = \"red\")\r\n",
    "plt.legend(loc=0)\r\n",
    "plt.xlabel('epochs')\r\n",
    "plt.ylabel('accuracies on dataset')\r\n",
    "plt.grid(True)\r\n",
    "plt.title(\"Training and validation accuracy\")\r\n",
    "plt.show()\r\n",
    "fig.savefig('documentation\\\\images\\\\accuracy_CUDA.jpg')\r\n",
    "plt.close(fig)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# summarize history for loss\r\n",
    "fig = plt.figure(figsize=(12,8))\r\n",
    "plt.plot(history.history['loss'],label='training loss')\r\n",
    "plt.plot(history.history['val_loss'],label='validation loss')\r\n",
    "plt.legend(loc=0)\r\n",
    "plt.ylabel('losses on dataset')\r\n",
    "plt.grid(True)\r\n",
    "plt.title(\"Training and validation loss\")\r\n",
    "plt.show()\r\n",
    "fig.savefig('documentation//images//loss_CUDA.jpg')\r\n",
    "plt.close(fig)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_json = model.to_json()\r\n",
    "with open('model66_CUDA.json', 'w') as json_file:\r\n",
    "    json_file.write(model_json)\r\n",
    "\r\n",
    "model.save('model66_CUDA.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load black and white images\r\n",
    "color_me = []\r\n",
    "for filename in os.listdir('test256/'):\r\n",
    "        color_me.append(img_to_array(load_img('test256/'+filename)))\r\n",
    "color_me = np.array(color_me, dtype=float)\r\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\r\n",
    "color_me = color_me.reshape(color_me.shape+(1,))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output = model.predict(color_me)\r\n",
    "output = output * 128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Output colorizations\r\n",
    "for i in range(len(output)):\r\n",
    "    result = np.zeros((256, 256, 3))\r\n",
    "    result[:,:,0] = color_me[i][:,:,0]\r\n",
    "    result[:,:,1:] = output[i]\r\n",
    "    imsave(\"result_h66_CUDA/uint8_\"+str(i)+\".png\", lab2rgb(result))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "in_hsv_h = color.convert_colorspace(output, 'RGB', 'HSV')\n",
    "in_hsv_s = in_hsv_h.copy()\n",
    "in_hsv_v = in_hsv_h.copy()\n",
    "\n",
    "for i in range(newImage.shape[0]):\n",
    "    in_hsv_h[i,:,0] = np.sort(in_hsv_h[i,:,0])\n",
    "    in_hsv_s[i,:,1] = np.sort(in_hsv_s[i,:,1])\n",
    "    in_hsv_v[i,:,2] = np.sort(in_hsv_v[i,:,2])\n",
    "\n",
    "imsave('testing-sorted-hue.png', (color.convert_colorspace(in_hsv_h, 'HSV', 'RGB')*255).astype(np.uint8))\n",
    "imsave('testing-sorted-saturation.png', (color.convert_colorspace(in_hsv_s, 'HSV', 'RGB')*255).astype(np.uint8))"
   ],
   "metadata": {}
  }
 ]
}